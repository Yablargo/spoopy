{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# Loading in the training data with Pandas\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "all_words = train['text'].str.split(expand=True).unstack().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [this, process, ,, however, ,, afforded, me, n...\n",
       "1        [it, never, once, occurred, to, me, that, the,...\n",
       "2        [in, his, left, hand, was, a, gold, snuff, box...\n",
       "3        [how, lovely, is, spring, as, we, looked, from...\n",
       "4        [finding, nothing, else, ,, not, even, gold, ,...\n",
       "5        [a, youth, passed, in, solitude, ,, my, best, ...\n",
       "6        [the, astronomer, ,, perhaps, ,, at, this, poi...\n",
       "7        [the, surcingle, hung, in, ribands, from, my, ...\n",
       "8        [i, knew, that, you, could, not, say, to, your...\n",
       "9        [i, confess, that, neither, the, structure, of...\n",
       "10       [he, shall, find, that, i, can, feel, my, inju...\n",
       "11       [here, we, barricaded, ourselves, ,, and, ,, f...\n",
       "12       [herbert, west, needed, fresh, bodies, because...\n",
       "13       [the, farm, like, grounds, extended, back, ver...\n",
       "14       [but, a, glance, will, show, the, fallacy, of,...\n",
       "15       [he, had, escaped, me, ,, and, i, must, commen...\n",
       "16       [to, these, speeches, they, gave, ,, of, cours...\n",
       "17       [her, native, sprightliness, needed, no, undue...\n",
       "18       [i, even, went, so, far, as, to, speak, of, a,...\n",
       "19       [his, facial, aspect, ,, too, ,, was, remarkab...\n",
       "20       [now, the, net, work, was, not, permanently, f...\n",
       "21       [it, was, not, that, the, sounds, were, hideou...\n",
       "22       [on, every, hand, was, a, wilderness, of, balc...\n",
       "23       [with, how, deep, a, spirit, of, wonder, and, ...\n",
       "24       [these, bizarre, attempts, at, explanation, we...\n",
       "25       [for, many, prodigies, and, signs, had, taken,...\n",
       "26       [all, that, as, yet, can, fairly, be, said, to...\n",
       "27       [i, seemed, to, be, upon, the, verge, of, comp...\n",
       "28       [our, compasses, ,, depth, gauges, ,, and, oth...\n",
       "29       [this, the, young, warriors, took, back, with,...\n",
       "                               ...                        \n",
       "19549    [but, it, was, not, so, ;, i, was, the, same, ...\n",
       "19550    [he, then, took, the, book, himself, ,, and, r...\n",
       "19551    [``, adolphe, le, bon, ,, clerk, to, mignaud, ...\n",
       "19552    [but, of, the, character, of, his, remarks, at...\n",
       "19553    [he, notes, every, variation, of, face, as, th...\n",
       "19554    [they, admitted, they, had, been, drunk, ,, bu...\n",
       "19555    [the, rays, of, the, newly, risen, sun, poured...\n",
       "19556    [to, the, north, on, the, craggy, precipice, a...\n",
       "19557    [the, frauds, of, the, banks, of, course, i, c...\n",
       "19558    [he, was, attired, ,, as, i, had, expected, ,,...\n",
       "19559    [when, a, fumbling, came, in, the, nearer, cas...\n",
       "19560    [but, then, there, is, the, tone, laconic, ,, ...\n",
       "19561    [average, people, in, society, and, business, ...\n",
       "19562    [the, modes, and, sources, of, this, kind, of,...\n",
       "19563    [yet, from, whom, has, not, that, rude, hand, ...\n",
       "19564    [almighty, god, no, ,, no, they, heard, they, ...\n",
       "19565    [i, hope, you, have, not, been, so, foolish, a...\n",
       "19566    [these, reflections, made, our, legislators, p...\n",
       "19567    [because, there, were, some, considerations, o...\n",
       "19568    [before, going, in, we, walked, up, the, stree...\n",
       "19569    [once, my, fancy, was, soothed, with, dreams, ...\n",
       "19570    [nay, ,, you, may, have, met, with, another, w...\n",
       "19571    [my, watch, was, still, going, ,, and, told, m...\n",
       "19572    [but, these, and, other, difficulties, attendi...\n",
       "19573    [stress, of, weather, drove, us, up, the, adri...\n",
       "19574    [i, could, have, fancied, ,, while, i, looked,...\n",
       "19575    [the, lids, clenched, themselves, together, as...\n",
       "19576    [mais, il, faut, agir, that, is, to, say, ,, a...\n",
       "19577    [for, an, item, of, news, like, this, ,, it, s...\n",
       "19578    [he, laid, a, gnarled, claw, on, my, shoulder,...\n",
       "Name: words, Length: 19579, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have a bunch of wonky punctuation that we need to fix, let's try again.\n",
    "\n",
    "train['words'] = train['text'].apply(lambda t: nltk.word_tokenize(str.lower(t))).values.tolist()\n",
    "train['words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#remove stopwords\n",
    "punctuation = [',','.',';','?',':','``',\"''\",\"'\"]\n",
    "stopwords = stopwords  + punctuation\n",
    "\n",
    "train['words'] = train['words'].apply(lambda t: [word for word in t if word not in stopwords and len(word) > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one             1623\n",
       "upon            1411\n",
       "'s              1355\n",
       "could           1330\n",
       "would           1258\n",
       "man              777\n",
       "time             730\n",
       "yet              715\n",
       "said             704\n",
       "even             700\n",
       "might            629\n",
       "old              616\n",
       "like             613\n",
       "first            602\n",
       "must             597\n",
       "us               596\n",
       "never            570\n",
       "life             569\n",
       "night            566\n",
       "made             565\n",
       "found            558\n",
       "seemed           544\n",
       "eyes             540\n",
       "every            535\n",
       "little           531\n",
       "day              523\n",
       "still            519\n",
       "great            511\n",
       "long             510\n",
       "saw              502\n",
       "                ... \n",
       "comforters         1\n",
       "mtal               1\n",
       "austrians          1\n",
       "sumner             1\n",
       "ecstacies          1\n",
       "sarmatic           1\n",
       "turvy              1\n",
       "homewards          1\n",
       "quainter           1\n",
       "physiologist       1\n",
       "attacking          1\n",
       "junctions          1\n",
       "acquaint           1\n",
       "necromancy         1\n",
       "unimpaired         1\n",
       "verborum           1\n",
       "subtends           1\n",
       "hub                1\n",
       "handsomely         1\n",
       "canyon             1\n",
       "relented           1\n",
       "orations           1\n",
       "gualtier           1\n",
       "alcmaeon           1\n",
       "transact           1\n",
       "bleus              1\n",
       "spending           1\n",
       "arrises            1\n",
       "revelling          1\n",
       "theophrastus       1\n",
       "Length: 25226, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all the words into a series and show the value counts of all the various words.\n",
    "slist = []\n",
    "for x in train['words']:\n",
    "        slist.extend(x)\n",
    "\n",
    "all_words = pd.Series(slist)\n",
    "counts = all_words.value_counts()\n",
    "#The most common word is still a blank string, and I'm not sure why this is an issue.\n",
    "counts\n",
    "\n",
    "#Train for bucket of words\n",
    "\n",
    "#Train for categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save shape for splitting later\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "\n",
    "# Get subset of data for each author\n",
    "train_EAP = train[train.author.isin(['EAP'])]\n",
    "train_MWS = train[train.author.isin(['MWS'])]\n",
    "train_HPL = train[train.author.isin(['HPL'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one-hot encoding\n",
    "author_one_hot = pd.get_dummies(train['author'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 28)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 65)\t1\n",
      "  (0, 66)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 68)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 70)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 72)\t1\n",
      "  (0, 73)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 77)\t1\n",
      "  (0, 78)\t1\n"
     ]
    }
   ],
   "source": [
    "#Make a vector from all words bucket\n",
    "\n",
    "unique_words = all_words.unique()\n",
    "cv = CountVectorizer(vocabulary=unique_words)\n",
    "output = cv.fit_transform(train['text'])\n",
    "out_arr = output\n",
    "#st = vec.fit_transform(train['words'])\n",
    "#st\n",
    "\n",
    "print(out_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, at this point, I have a matrix of each individual word for each row in train['text'] it is in the var output\n",
    "\n",
    "From here, I can do regular old regression against the various authors (one at a time, I presume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = pd.DataFrame(output.toarray())\n",
    "train_matrix['id'] = train['id']\n",
    "\n",
    "\n",
    "#Now, I have a training matrix with everything, but the author isn't categorized out yet.\n",
    "train_matrix[0:5]\n",
    "\n",
    "#I want to get three separate models for guessing EAP, MHS, HPL, and setup three separate matrixes to do so.\n",
    "\n",
    "EAP_matrix = train_matrix.copy()\n",
    "MWS_matrix = train_matrix.copy()\n",
    "HPL_matrix = train_matrix.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    1\n",
      "7    1\n",
      "8    1\n",
      "9    0\n",
      "Name: EAP, dtype: int64\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "5    1\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    1\n",
      "Name: MWS, dtype: int64\n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    0\n",
      "Name: HPL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def getAuthorValue(outMatrix,inColumn,strMatch):\n",
    "    outMatrix[strMatch] = inColumn.apply(lambda t: 1 if t == strMatch else 0)    \n",
    "\n",
    "#Get the author values for each training matrix\n",
    "#EAP_matrix['EAP'] = train['author'].apply(lambda t: 1 if t == 'EAP' else 0)\n",
    "getAuthorValue(EAP_matrix,train['author'],'EAP')\n",
    "getAuthorValue(MWS_matrix,train['author'],'MWS')\n",
    "getAuthorValue(HPL_matrix,train['author'],'HPL')\n",
    "print(EAP_matrix['EAP'][0:10])\n",
    "print(MWS_matrix['MWS'][0:10])\n",
    "print(HPL_matrix['HPL'][0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok! We have working matrixes that look right for each train set. We now want to setup 3 different regression models for each author\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
