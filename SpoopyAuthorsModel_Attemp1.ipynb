{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import nltk\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save shape for splitting later\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subset of data for each author\n",
    "train_EAP = train[train.author.isin(['EAP'])]\n",
    "train_MWS = train[train.author.isin(['MWS'])]\n",
    "train_HPL = train[train.author.isin(['HPL'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EAP    7900\n",
       "MWS    6044\n",
       "HPL    5635\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe remove punctuation? Is \"help, different than help different than \"help different than help, ?\n",
    "\n",
    "This will be done with tokenization, actually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = train['text'].str.split(expand=True).unstack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_EAP = train_EAP['text'].str.split(expand=True).unstack().value_counts()\n",
    "all_words_MWS = train_MWS['text'].str.split(expand=True).unstack().value_counts()\n",
    "all_words_HPL = train_HPL['text'].str.split(expand=True).unstack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the               13927\n",
       "of                 8930\n",
       "and                5222\n",
       "to                 4625\n",
       "a                  4514\n",
       "in                 3750\n",
       "I                  3598\n",
       "was                2109\n",
       "that               2085\n",
       "my                 1666\n",
       "with               1635\n",
       "it                 1555\n",
       "is                 1530\n",
       "at                 1464\n",
       "as                 1458\n",
       "which              1377\n",
       "had                1265\n",
       "for                1214\n",
       "not                1184\n",
       "his                1167\n",
       "by                 1125\n",
       "be                 1049\n",
       "have               1037\n",
       "this                964\n",
       "he                  963\n",
       "The                 954\n",
       "upon                951\n",
       "from                938\n",
       "but                 803\n",
       "an                  783\n",
       "                  ...  \n",
       "jacket                1\n",
       "monotonously          1\n",
       "\"'We                  1\n",
       "disorders             1\n",
       "vegetation,           1\n",
       "regretted.            1\n",
       "defying               1\n",
       "testimony,            1\n",
       "testimony.            1\n",
       "\"Several              1\n",
       "silly,                1\n",
       "Amriccan              1\n",
       "assented              1\n",
       "Mediterranean;        1\n",
       "Tremaine,             1\n",
       "foaming,              1\n",
       "Witness               1\n",
       "\"Hem\"                 1\n",
       "elderly               1\n",
       "\"playing              1\n",
       "Tremaine?             1\n",
       "palaces.\"             1\n",
       "murderer,             1\n",
       "observe.\"             1\n",
       "Pressing              1\n",
       "manner.\"              1\n",
       "thinner               1\n",
       "jealousy              1\n",
       "prepared.             1\n",
       "overawed              1\n",
       "Length: 26521, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_EAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process , however , afforded me no means of ascertaining the dimensions of my dungeon ; as I might make its circuit , and return to the point whence I set out , without being aware of the fact ; so perfectly uniform seemed the wall . In his\n"
     ]
    }
   ],
   "source": [
    "# All tokens of each author\n",
    "all_tokens = nltk.word_tokenize(\" \".join(train.text.values).decode('utf8'))\n",
    "all_tokens_EAP = nltk.word_tokenize(\" \".join(train_EAP.text.values).decode('utf8'))\n",
    "all_tokens_MWS = nltk.word_tokenize(\" \".join(train_MWS.text.values).decode('utf8'))\n",
    "all_tokens_HPL = nltk.word_tokenize(\" \".join(train_HPL.text.values).decode('utf8'))\n",
    "print(\" \".join(all_tokens_EAP[0:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'This', u'process', u',', u'however', u',', u'afforded', u'me', u'no', u'means', u'of', u'ascertaining', u'the', u'dimensions', u'of', u'my', u'dungeon', u';', u'as', u'I', u'might', u'make', u'its', u'circuit', u',', u'and', u'return', u'to', u'the', u'point', u'whence', u'I', u'set', u'out', u',', u'without', u'being', u'aware', u'of', u'the', u'fact', u';', u'so', u'perfectly', u'uniform', u'seemed', u'the', u'wall', u'.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# All tokens of each author, spearated by rows, appended to the original dataframe\n",
    "train['tokens'] = [nltk.word_tokenize(i.decode('utf8')) for i in train.text.values]\n",
    "train_EAP['tokens'] = [nltk.word_tokenize(i.decode('utf8')) for i in train_EAP.text.values]\n",
    "train_MWS['tokens'] = [nltk.word_tokenize(i.decode('utf8')) for i in train_MWS.text.values]\n",
    "train_HPL['tokens'] = [nltk.word_tokenize(i.decode('utf8')) for i in train_HPL.text.values]\n",
    "print(train_EAP.tokens.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'thi', u'process', u',', u'howev', u',', u'afford', u'me', u'no', u'mean', u'of', u'ascertain', u'the', u'dimens', u'of', u'my', u'dungeon', u';', u'a', u'I', u'might', u'make', u'it', u'circuit', u',', u'and', u'return', u'to', u'the', u'point', u'whenc', u'I', u'set', u'out', u',', u'without', u'be', u'awar', u'of', u'the', u'fact', u';', u'so', u'perfectli', u'uniform', u'seem', u'the', u'wall', u'.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Stem/lemm the tokens\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "train['lemmed'] = [[stemmer.stem(lemmer.lemmatize(j)) for j in i] for i in train.tokens.values ]\n",
    "train_EAP['lemmed'] = [[stemmer.stem(lemmer.lemmatize(j)) for j in i] for i in train_EAP.tokens.values]\n",
    "train_MWS['lemmed'] = [[stemmer.stem(lemmer.lemmatize(j)) for i in i] for i in train_MWS.tokens.values]\n",
    "train_HPL['lemmed'] = [[stemmer.stem(lemmer.lemmatize(j)) for j in i] for i in train_HPL.tokens.values]\n",
    "print(train_EAP.lemmed.values[0])\n",
    "\n",
    "\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",             17594\n",
       "the           14969\n",
       "of             8938\n",
       ".              7700\n",
       "a              5997\n",
       "and            5733\n",
       "to             4646\n",
       "I              3775\n",
       "in             3773\n",
       "it             2491\n",
       "that           2327\n",
       "wa             2229\n",
       "with           1695\n",
       "my             1670\n",
       "is             1645\n",
       "``             1628\n",
       "which          1488\n",
       "at             1468\n",
       "''             1359\n",
       ";              1354\n",
       "not            1347\n",
       "for            1343\n",
       "had            1318\n",
       "be             1311\n",
       "thi            1308\n",
       "hi             1284\n",
       "have           1242\n",
       "but            1200\n",
       "by             1144\n",
       "upon           1025\n",
       "              ...  \n",
       "rôle              1\n",
       "verbal            1\n",
       "contr             1\n",
       "bedizzen          1\n",
       "unsoci            1\n",
       "allbreath         1\n",
       "kickapo           1\n",
       "widdi             1\n",
       "wheat             1\n",
       "sadden            1\n",
       "bethink           1\n",
       "engross           1\n",
       "treacher          1\n",
       "bra               1\n",
       "wilkin            1\n",
       "tibia             1\n",
       "aurelian          1\n",
       "milki             1\n",
       "snow              1\n",
       "leafi             1\n",
       "senseless         1\n",
       "hxmx              1\n",
       "angler            1\n",
       "pauvr             1\n",
       "frown             1\n",
       "vertu             1\n",
       "basest            1\n",
       "unwrought         1\n",
       "blackberri        1\n",
       "fawn              1\n",
       "Length: 10346, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([j for i in train_EAP.lemmed.values for j in i]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EAP  HPL  MWS\n",
       "0    1    0    0\n",
       "1    0    1    0\n",
       "2    1    0    0\n",
       "3    0    0    1\n",
       "4    0    1    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get one-hot encoding\n",
    "author_one_hot = pd.get_dummies(train['author'])\n",
    "author_one_hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>EAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  EAP\n",
       "0  id26305  This process, however, afforded me no means of...    1\n",
       "1  id17569  It never once occurred to me that the fumbling...    0\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    1\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    0\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make 3 train sets from one hot\n",
    "train_is_EAP = train[['id','text']].join(author_one_hot[['EAP']])\n",
    "train_is_MWS = train[['id','text']].join(author_one_hot[['MWS']])\n",
    "train_is_HPL = train[['id','text']].join(author_one_hot[['HPL']])\n",
    "train_is_EAP.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------Random attempt-------------\n",
    "\n",
    "# Split training data so we have sove test data to validate our model\n",
    "# Use test size of 30%\n",
    "X_train_EAP, X_test_EAP, y_train_EAP, y_test_EAP = train_test_split(train_is_EAP.text, \n",
    "                                                                    train_is_EAP.EAP,\n",
    "                                                                    test_size=.3,\n",
    "                                                                    random_state=0)\n",
    "# Set up a pipeline to tokenize and then estimate\n",
    "pipe1 = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# Set up parameters to test in our grid search\n",
    "parameters = {\n",
    "    'cv__stop_words': [None,'english'],\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__loss':['log']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   14.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Test all combinations of paramters for our estimator and vectorizer\n",
    "gs = GridSearchCV(pipe1, parameters, n_jobs=-1, verbose=1)\n",
    "gs = gs.fit(X_train_EAP, y_train_EAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 1e-05,\n",
       " 'clf__loss': 'log',\n",
       " 'clf__penalty': 'l2',\n",
       " 'cv__stop_words': None}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_     # The best combination of parameters for our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16527    0.0\n",
      "6398     0.0\n",
      "10604    0.0\n",
      "17864    0.0\n",
      "19368    0.0\n",
      "15369    1.0\n",
      "3949     0.0\n",
      "14934    0.0\n",
      "5469     1.0\n",
      "6409     1.0\n",
      "dtype: float64\n",
      "\n",
      "16527    0\n",
      "6398     0\n",
      "10604    0\n",
      "17864    0\n",
      "19368    0\n",
      "15369    0\n",
      "3949     1\n",
      "14934    0\n",
      "5469     1\n",
      "6409     1\n",
      "Name: EAP, dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "# Get a prediction from the test data we split off earlier -- We have the correct labels for this in y_test_EAP\n",
    "y_pred_EAP = gs.predict_proba(X_test_EAP)\n",
    "\n",
    "# Reformat results\n",
    "y_pred_EAP = np.array(y_pred_EAP)\n",
    "y_pred_EAP = pd.Series(index=y_test_EAP.index,\n",
    "                       data=y_pred_EAP[:,1]\n",
    "                      ).round()\n",
    "\n",
    "print(y_pred_EAP.head(10))\n",
    "print\n",
    "print(y_test_EAP.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5874\n",
       "unique       2\n",
       "top       True\n",
       "freq      4732\n",
       "dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Results -----\n",
    "y_test_EAP.eq(y_pred_EAP).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8055839291794348"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4732.0/5874.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7149907991735525"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log_loss is the specified metric from the competition\n",
    "log_loss(y_test_EAP,y_pred_EAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know why log loss is so high. I may be inputing the results wrong. Or maybe it's because I haven't done the other authors yet. Also, note the `.round()` when formatting `y_pred_EAP`. The actual results had floats that were effectively 0 (e-14,15,16 etc) or very nearly 1 (e.g 9.99807e-1). In rounding, there is inherently some information loss so maybe we can find a math function to map that value to some sort of confidence in the result.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
